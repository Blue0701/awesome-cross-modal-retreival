# Awesome Multimodal LLMs & Encoders üöÄ  

A curated list of state-of-the-art **multimodal representation learning models and encoders** for text, image, audio, and video processing.  

### Includes:  
- Multimodal LLMs  
- Encoders for Text, Image, Audio, and Video  




### üèÜ **2025**  
| Modality | Conference | Paper | Code |
|----------|------------|-----------------------------------------------------------|------|
| I, V     | arXiv      | [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub](https://github.com/ictnlp/LLaVA-Mini?tab=readme-ov-file) |
| I, V     | arXiv      | [VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub](https://github.com/DAMO-NLP-SG/VideoLLaMA3?tab=readme-ov-file) |
| I, A     | WACV 2025  | [TaxaBind: A Unified Embedding Space for Ecological Applications](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub](https://github.com/mvrl/taxabind?tab=readme-ov-file) |

---

### üèÜ **2024**  
| Modality | Conference | Paper | Code |
|----------|------------|------------------------------------------------------|------|
| I, V, A, P | CVPR 2024 | [OneLLM: One Framework to Align All Modalities with Language](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub](https://github.com/csuhan/OneLLM?tab=readme-ov-file) |
| I, V, A, P | WACV 2024 | [OmniVec: Learning Robust Representations With Cross Modal Sharing](https://arxiv.org/abs/XXXX.XXXXX) | - |

---

### üèÜ **2023**  
| Modality | Conference | Paper | Code |
|----------|------------|------------------------------------------------------|------|
| I, V     | arXiv      | [EVA-CLIP: Improved Training Techniques for CLIP at Scale](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub](https://github.com/baaivision/eva) |
| I, V, A  | CVPR 2023  | [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub](https://github.com/facebookresearch/imagebind) |
| I, V, A  | NIPS 2023  | [VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub](https://github.com/txh-mercury/vast) |

---

### üèÜ **2021**  
| Modality | Conference | Paper | Code |
|----------|------------|----------------------------------------------------------------|------|
| I        | OpenAI     | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub](https://github.com/openai/CLIP?tab=readme-ov-file) |

---
