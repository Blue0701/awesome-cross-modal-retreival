# Awesome Multimodal representation

## 2025
|modality|conference|paper|code|
|------|---|---|---|
|I, V|arxiv|LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token|[code](https://github.com/ictnlp/LLaVA-Mini?tab=readme-ov-file)
|I, V|arxiv|VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding|[code](https://github.com/DAMO-NLP-SG/VideoLLaMA3?tab=readme-ov-file)
|I, A|WACV 2025|TaxaBind: A Unified Embedding Space for Ecological Applications|[code](https://github.com/mvrl/taxabind?tab=readme-ov-file)

## 2024
|modality|conference|paper|code|
|------|---|---|---|
|I, V, A, P|CVPR 2024|OneLLM: One Framework to Align All Modalities with Language|[code](https://github.com/csuhan/OneLLM?tab=readme-ov-file)  
|I, V, A, P|WACV 2024|OmniVec: Learning Robust Representations With Cross Modal Sharing|-  




## 2023
|modality|conference|paper|code|
|------|---|---|---|
|I, V|arxiv|EVA-CLIP: Improved Training Techniques for CLIP at Scale|[code](https://github.com/baaivision/eva)  
|I, V, A|CVPR 2023|ImageBind: One Embedding Space To Bind Them All|[code](https://github.com/facebookresearch/imagebind)  
|I, V, A|NIPS 2023|VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset|[code](https://github.com/txh-mercury/vast)  


## 2021
|modality|conference|paper|code|
|------|---|---|---|
|I|openAI|Learning Transferable Visual Models From Natural Language Supervision|[code](https://github.com/openai/CLIP?tab=readme-ov-file)  




